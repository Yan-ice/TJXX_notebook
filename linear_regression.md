# 线性回归（Linear Regression）

## 1. 概述
线性回归是一种**监督学习**算法，主要用于**回归问题**。它通过建立自变量（输入特征）和因变量（输出目标）之间的线性关系来进行预测。线性回归的目标是通过找到最佳拟合直线，使得预测值与实际值之间的误差最小化。

线性回归模型可以表示为：
\[
y = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
\]
其中，\( x_1, x_2, \dots, x_n \) 是输入特征，\( w_1, w_2, \dots, w_n \) 是模型的权重参数，\( b \) 是偏置项，\( y \) 是输出预测值。

## 2. 线性回归的种类
### 2.1 简单线性回归
简单线性回归只涉及一个自变量和一个因变量，其模型形式为：
\[
y = w_1 x + b
\]
目标是找到 \( w_1 \) 和 \( b \)，使得数据点与直线的误差最小。

### 2.2 多元线性回归
多元线性回归涉及多个自变量，其模型形式为：
\[
y = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
\]
这种情况下，模型要学到多个权重 \( w_1, w_2, \dots, w_n \) 和偏置 \( b \)。

## 3. 损失函数
线性回归的目标是找到一组参数 \( w \) 和 \( b \)，使得预测值 \( \hat{y} \) 与真实值 \( y \) 之间的误差最小。通常使用**均方误差（Mean Squared Error, MSE）**作为损失函数，其定义为：
\[
MSE = \frac{1}{m} \sum_{i=1}^m (\hat{y_i} - y_i)^2
\]
其中：
- \( m \) 是样本的数量；
- \( \hat{y_i} \) 是第 \( i \) 个样本的预测值；
- \( y_i \) 是第 \( i \) 个样本的实际值。

MSE 反映了预测值与实际值之间的偏离程度，越小越好。

## 4. 参数估计：最小二乘法
在线性回归中，最常用的参数估计方法是**最小二乘法**（Ordinary Least Squares, OLS）。最小二乘法通过最小化均方误差来估计模型的参数。对于多元线性回归，最小二乘法的解析解可以表示为：
\[
\hat{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}
\]

通过这个公式，可以计算出使得损失函数最小的最优权重。

```
定义：
特征矩阵（Feature Matrix）是机器学习或数据分析中常用的术语，表示模型的输入数据，也就是自变量（特征）的矩阵。每一行代表一个样本，每一列代表一个特征。
设计矩阵（Design Matrix）是统计学中常用的术语，特别是在回归分析中，描述的是模型的自变量（或解释变量）和因变量的关系。它通常用于表征线性模型中的变量，并用于构建估计参数的线性方程。
在线性回归模型中，设计矩阵通常包括一个全为 1 的列来表示偏置项（截距）.
```

#### 期望
在经典线性回归模型的假设下，最小二乘估计量 \( \hat{\beta} \) 的期望为：
\[
\mathbb{E}(\hat{\beta}) = \beta
\]
这表示在多个样本中进行估计时，最小二乘法的估计量是无偏的。

#### 方差
最小二乘估计量的方差可以表示为：
\[
\text{Var}(\hat{\beta}) = \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
\]
其中：
- \( \sigma^2 \) 是误差项的方差；
- \( (\mathbf{X}^T \mathbf{X})^{-1} \) 是自变量设计矩阵的逆。



## 5. 假设与前提条件
线性回归模型依赖于以下几个假设：
1. **线性关系**：自变量与因变量之间存在线性关系。
2. **独立性**：样本之间是相互独立的。
3. **同方差性**：不同自变量的误差项具有相同的方差，即误差项的散布与自变量的取值无关。
4. **正态性**：误差项服从正态分布。
5. **多重共线性**：在多元线性回归中，自变量之间的相关性应尽量低，否则可能会引起多重共线性问题，影响模型的稳定性。

## 6. 评价指标
线性回归模型的效果可以通过以下几个常见的指标来评价：
- **R² 决定系数**：R² 衡量模型解释数据方差的能力，取值范围在 0 到 1 之间，值越大说明模型拟合越好。其定义为：
  \[
  R^2 = 1 - \frac{\sum_{i=1}^m (\hat{y_i} - y_i)^2}{\sum_{i=1}^m (y_i - \bar{y})^2}
  \]
  其中 \( \bar{y} \) 是实际值的均值。
  
- **均方误差（MSE）**：如前所述，MSE 用于衡量预测值与实际值之间的平均误差，值越小越好。

- **均方根误差（RMSE）**：RMSE 是 MSE 的平方根，其单位与因变量相同，直观反映了预测误差的大小。
  \[
  RMSE = \sqrt{\frac{1}{m} \sum_{i=1}^m (\hat{y_i} - y_i)^2}
  \]

- **平均绝对误差（MAE）**：MAE 计算预测值与实际值之间的绝对误差的平均值：
  \[
  MAE = \frac{1}{m} \sum_{i=1}^m |\hat{y_i} - y_i|
  \]

## 7. 线性回归的优缺点
### 优点：
- **简单易实现**：线性回归算法简单，容易解释和实现。
- **高效**：对于小规模数据集，线性回归计算效率非常高。
- **可解释性强**：线性回归的模型参数具有较好的可解释性，可以清晰地说明特征与目标变量之间的关系。
  
### 缺点：
- **线性假设限制**：线性回归要求自变量与因变量之间必须具有线性关系，无法处理非线性关系的数据。
- **对异常值敏感**：线性回归容易受到异常值的影响，因为异常值会显著影响拟合的结果。
- **多重共线性问题**：在多元线性回归中，若特征之间存在较强的相关性，会导致参数估计不稳定。

## 8. 改进与扩展
为了解决线性回归的一些局限性，出现了许多扩展方法：
- **岭回归（Ridge Regression）**：通过对权重引入 \( L2 \) 正则化，来减少多重共线性对模型的影响。
- **Lasso 回归**：通过对权重引入 \( L1 \) 正则化，使部分权重归零，从而实现特征选择。
- **弹性网回归（Elastic Net）**：结合了岭回归和 Lasso 回归的优点，同时使用 \( L1 \) 和 \( L2 \) 正则化。



### 岭回归的损失函数
岭回归的损失函数在普通最小二乘回归（OLS）的基础上添加了 \( L2 \) 正则化项：

\[
J(w) = \sum_{i=1}^m (y_i - \hat{y_i})^2 + \lambda \sum_{j=1}^n w_j^2
\]

其中：
- \( \hat{y_i} = w^T x_i + b \) 是第 \( i \) 个样本的预测值；
- \( \lambda \) 是正则化参数，控制模型的正则化强度；
- \( w_j \) 是模型的权重参数；
- \( \sum_{j=1}^n w_j^2 \) 是权重的平方和。

or, 岭回归的解析解为：
\[
\hat{\beta}_{\text{ridge}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{Y}
\]
其中 \( \mathbf{I} \) 是单位矩阵。

### Lasso 回归的损失函数
Lasso 回归的损失函数在普通最小二乘回归（OLS）的基础上添加了 \( L1 \) 正则化项：

\[
J(w) = \sum_{i=1}^m (y_i - \hat{y_i})^2 + \lambda \sum_{j=1}^n |w_j|
\]

其中：
- \( \hat{y_i} = w^T x_i + b \) 是第 \( i \) 个样本的预测值；
- \( \lambda \) 是正则化参数，控制模型的正则化强度；
- \( \sum_{j=1}^n |w_j| \) 是权重的绝对值和。

or, Lasso 回归的目标函数可以表示为：
\[
\hat{\beta}_{\text{lasso}} = \underset{\beta}{\text{argmin}} \left( \left\| \mathbf{Y} - \mathbf{X} \beta \right\|^2_2 + \lambda \left\| \beta \right\|_1 \right)
\]
