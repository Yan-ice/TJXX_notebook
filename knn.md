## KNN
K-Nearest Neighbour (KNN) 是一种基于实例的监督学习算法，广泛用于分类和回归任务。KNN 算法通过在给定输入样本空间中查找与目标样本最接近的训练样本，来进行预测。它简单易懂，且具有较高的可解释性。以下是对 KNN 的详细介绍：

1. 基本原理
KNN 的核心思想是：相似的样本往往属于同一类。当我们预测一个新样本时，算法通过计算该样本与训练集中所有样本的距离，找到距离最近的 K 个邻居，并根据这些邻居的类别信息进行分类或回归预测。

2. 工作流程
KNN 算法的主要步骤如下：

**确定参数K**: 即要考虑的最近邻居的个数。
计算距离：使用合适的距离度量（如欧几里得距离、曼哈顿距离等），计算待分类样本与训练集中每个样本之间的距离。
选择 K 个最近邻居：按照距离从小到大排序，选择距离最近的 K 个训练样本。
做出决策：
分类任务：统计 K 个邻居中各类别的频次，类别频次最高的即为待分类样本的类别。
回归任务：取 K 个邻居的数值平均值，作为预测结果。
3. 距离度量
KNN 中常用的距离度量有：

欧几里得距离 (Euclidean Distance)：适用于连续数值型特征。
曼哈顿距离 (Manhattan Distance)：即特征值之差的绝对值之和.
闵可夫斯基距离 (Minkowski Distance)：欧几里得和曼哈顿的广义化.

其中，p=2 为欧几里得距离，p=1 为曼哈顿距离。
4. 选择合适的 K 值
K 值过小：模型可能过拟合，对噪声敏感。
K 值过大：模型可能欠拟合，邻居太多，类别边界变得模糊。
通常，K 是一个小的奇数（如 3、5、7 等），并通过交叉验证选择最优的 K 值。

5. KNN 的优缺点
优点：
简单直观：无需复杂的训练过程，只需存储训练样本即可。
无需参数估计：相比其他算法，KNN 不需要假设数据的分布。
对噪声有一定的鲁棒性（通过选择合适的 K 值）。
缺点：
计算复杂度高：每次预测都需要计算待分类样本与所有训练样本的距离，数据集较大时，时间和空间开销较大。
对维度敏感：随着特征维度的增加，计算距离时样本间的差异变得不明显，即“维度灾难”。
无法处理缺失数据：KNN 要求所有样本都有完整的特征向量。

6. KNN 的改进
加权 KNN：在决策时，给距离较近的邻居赋予更大的权重。
KD 树 或 球树：用于加速 KNN 查询，减少计算复杂度。
降维方法：如主成分分析（PCA）等用于减小特征空间，减轻维度灾难。
KNN 虽然简单，但在合适的应用场景下仍然是一个非常有效的算法。